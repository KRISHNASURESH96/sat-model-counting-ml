{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "This project addresses the problem of Algorithm Selection for Model Counting (#SAT), where the goal is to predict the most efficient solver for a given #SAT instance using machine learning techniques. We use a labeled dataset of 1337 SAT instances with 72 statistical and structural features to classify the best-performing solver out of five candidates: `gpmc`, `d4`, `ganak`, `addmc`, and `sharpsat`.\n",
    "\n",
    "### Tasks:\n",
    "- Understand the dataset and baseline model performance using k-NN.\n",
    "- Improve model performance using:\n",
    "  - Feature scaling (StandardScaler, MinMaxScaler)\n",
    "  - Dimensionality reduction (PCA)\n",
    "  - Feature selection (SelectKBest, SelectPercentile)\n",
    "  - Hyperparameter tuning (GridSearchCV)\n",
    "  - Cross-validation techniques\n",
    "- Compare performance with an alternate classifier: Support Vector Machine (SVM)\n",
    "\n",
    "We conclude by selecting and justifying the best-performing model based on accuracy and robustness.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WfrCFmLHxYu"
   },
   "source": [
    "\n",
    "### Objective\n",
    "\n",
    "The Boolean satisfiability (SAT) problem consists in determining whether a Boolean formula is satisfiable or not. This problem is one of the most widely studied combinatorial problems in computer science. It is the classic NP-complete problem. Over the past number of decades, a significant amount of research work has focused on solving SAT problems with both complete and incomplete solvers.\n",
    "\n",
    "An extended version of the problem is Model Counting (#SAT). In #SAT the solver needs to compute the number of solutions of a Boolean formula. A wide variety of solvers have been designed to tackle this problem.\n",
    "\n",
    "In this project, we want to create an Algorithm Selection (AS) approach to Model Counting. For each #SAT instance, there is a specific solver that works better than the others, the goal of your machine learing approach is to classify it.\n",
    "\n",
    "In AS we represent #SAT problems with a vector of 72 features with general information about the problem, e.g., number of variables, number of clauses, etc. There is no need to understand the features to be able to complete the assignment. For each instance, there is a 'label' column representing the name of the optimal solver.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oav9G1WSJ1nH"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Data Loading and Initial Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "DE0kM0QsJ1En",
    "outputId": "cd5865dc-60f3-4b3f-807a-bf1fdc60d94f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-8f7bfb02-7960-4a51-97df-ad520b47b911\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c</th>\n",
       "      <th>v</th>\n",
       "      <th>clauses_vars_ratio</th>\n",
       "      <th>vars_clauses_ratio</th>\n",
       "      <th>vcg_var_mean</th>\n",
       "      <th>vcg_var_coeff</th>\n",
       "      <th>vcg_var_min</th>\n",
       "      <th>vcg_var_max</th>\n",
       "      <th>vcg_var_entropy</th>\n",
       "      <th>vcg_clause_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>gsat_FirstLocalMinStep_CoeffVariance</th>\n",
       "      <th>gsat_FirstLocalMinStep_Median</th>\n",
       "      <th>gsat_FirstLocalMinStep_Q.10</th>\n",
       "      <th>gsat_FirstLocalMinStep_Q.90</th>\n",
       "      <th>gsat_BestAvgImprovement_Mean</th>\n",
       "      <th>gsat_BestAvgImprovement_CoeffVariance</th>\n",
       "      <th>gsat_FirstLocalMinRatio_Mean</th>\n",
       "      <th>gsat_FirstLocalMinRatio_CoeffVariance</th>\n",
       "      <th>gsat_EstACL_Mean</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>681.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>2.861345</td>\n",
       "      <td>0.349486</td>\n",
       "      <td>0.011143</td>\n",
       "      <td>0.905300</td>\n",
       "      <td>0.005874</td>\n",
       "      <td>0.111601</td>\n",
       "      <td>1.880038</td>\n",
       "      <td>0.011143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210148</td>\n",
       "      <td>50.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.954568</td>\n",
       "      <td>0.591296</td>\n",
       "      <td>1.141656</td>\n",
       "      <td>3.197217</td>\n",
       "      <td>9.240515e+09</td>\n",
       "      <td>gpmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>368.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>2.628571</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.018012</td>\n",
       "      <td>0.510753</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>1.851609</td>\n",
       "      <td>0.018012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124438</td>\n",
       "      <td>34.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.693036</td>\n",
       "      <td>0.244951</td>\n",
       "      <td>0.969015</td>\n",
       "      <td>0.029930</td>\n",
       "      <td>5.401642e+03</td>\n",
       "      <td>d4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1935.0</td>\n",
       "      <td>1920.0</td>\n",
       "      <td>1.007812</td>\n",
       "      <td>0.992248</td>\n",
       "      <td>0.001760</td>\n",
       "      <td>1.723720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012403</td>\n",
       "      <td>1.280404</td>\n",
       "      <td>0.001760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066708</td>\n",
       "      <td>102.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.398129</td>\n",
       "      <td>0.824694</td>\n",
       "      <td>0.935730</td>\n",
       "      <td>0.092714</td>\n",
       "      <td>3.561823e+04</td>\n",
       "      <td>gpmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3452.0</td>\n",
       "      <td>2821.0</td>\n",
       "      <td>1.223680</td>\n",
       "      <td>0.817207</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>1.436774</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.006083</td>\n",
       "      <td>1.192878</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053628</td>\n",
       "      <td>192.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>0.247528</td>\n",
       "      <td>0.702251</td>\n",
       "      <td>0.923327</td>\n",
       "      <td>0.026977</td>\n",
       "      <td>1.268929e+05</td>\n",
       "      <td>gpmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>694.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>2.360544</td>\n",
       "      <td>0.423631</td>\n",
       "      <td>0.007656</td>\n",
       "      <td>0.493513</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>0.040346</td>\n",
       "      <td>1.776102</td>\n",
       "      <td>0.007656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086841</td>\n",
       "      <td>72.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.822829</td>\n",
       "      <td>0.209989</td>\n",
       "      <td>0.855568</td>\n",
       "      <td>0.045802</td>\n",
       "      <td>1.647598e+04</td>\n",
       "      <td>d4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>949.0</td>\n",
       "      <td>351.0</td>\n",
       "      <td>2.703704</td>\n",
       "      <td>0.369863</td>\n",
       "      <td>0.006701</td>\n",
       "      <td>1.151888</td>\n",
       "      <td>0.002012</td>\n",
       "      <td>0.063380</td>\n",
       "      <td>1.857919</td>\n",
       "      <td>0.006701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079108</td>\n",
       "      <td>81.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.781888</td>\n",
       "      <td>0.256505</td>\n",
       "      <td>0.837929</td>\n",
       "      <td>0.066291</td>\n",
       "      <td>1.643030e+04</td>\n",
       "      <td>sharpsat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>1450.0</td>\n",
       "      <td>608.0</td>\n",
       "      <td>2.384868</td>\n",
       "      <td>0.419310</td>\n",
       "      <td>0.004427</td>\n",
       "      <td>1.552830</td>\n",
       "      <td>0.002408</td>\n",
       "      <td>0.161349</td>\n",
       "      <td>1.659495</td>\n",
       "      <td>0.004427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062664</td>\n",
       "      <td>136.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>0.776364</td>\n",
       "      <td>0.174557</td>\n",
       "      <td>0.855907</td>\n",
       "      <td>0.031698</td>\n",
       "      <td>3.816192e+04</td>\n",
       "      <td>gpmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>250.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.555766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>1.751533</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140708</td>\n",
       "      <td>26.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.093007</td>\n",
       "      <td>0.117640</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.168220e+03</td>\n",
       "      <td>gpmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>4949.0</td>\n",
       "      <td>3422.0</td>\n",
       "      <td>1.446230</td>\n",
       "      <td>0.691453</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>0.770643</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.004445</td>\n",
       "      <td>1.835041</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032571</td>\n",
       "      <td>461.0</td>\n",
       "      <td>441.0</td>\n",
       "      <td>479.0</td>\n",
       "      <td>0.202805</td>\n",
       "      <td>0.220101</td>\n",
       "      <td>0.856952</td>\n",
       "      <td>0.016884</td>\n",
       "      <td>6.236767e+05</td>\n",
       "      <td>gpmc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>280.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.018442</td>\n",
       "      <td>0.490373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045198</td>\n",
       "      <td>1.773180</td>\n",
       "      <td>0.018442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120101</td>\n",
       "      <td>35.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.982299</td>\n",
       "      <td>0.141661</td>\n",
       "      <td>0.995087</td>\n",
       "      <td>0.008575</td>\n",
       "      <td>6.045684e+03</td>\n",
       "      <td>gpmc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1336 rows × 73 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8f7bfb02-7960-4a51-97df-ad520b47b911')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-8f7bfb02-7960-4a51-97df-ad520b47b911 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-8f7bfb02-7960-4a51-97df-ad520b47b911');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-77443504-da47-447c-a0e2-706ec879f6f2\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-77443504-da47-447c-a0e2-706ec879f6f2')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-77443504-da47-447c-a0e2-706ec879f6f2 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "           c       v  clauses_vars_ratio  vars_clauses_ratio  vcg_var_mean  \\\n",
       "0      681.0   238.0            2.861345            0.349486      0.011143   \n",
       "1      368.0   140.0            2.628571            0.380435      0.018012   \n",
       "2     1935.0  1920.0            1.007812            0.992248      0.001760   \n",
       "3     3452.0  2821.0            1.223680            0.817207      0.000968   \n",
       "4      694.0   294.0            2.360544            0.423631      0.007656   \n",
       "...      ...     ...                 ...                 ...           ...   \n",
       "1331   949.0   351.0            2.703704            0.369863      0.006701   \n",
       "1332  1450.0   608.0            2.384868            0.419310      0.004427   \n",
       "1333   250.0   100.0            2.500000            0.400000      0.026000   \n",
       "1334  4949.0  3422.0            1.446230            0.691453      0.000764   \n",
       "1335   280.0   120.0            2.333333            0.428571      0.018442   \n",
       "\n",
       "      vcg_var_coeff  vcg_var_min  vcg_var_max  vcg_var_entropy  \\\n",
       "0          0.905300     0.005874     0.111601         1.880038   \n",
       "1          0.510753     0.005435     0.054348         1.851609   \n",
       "2          1.723720     0.000000     0.012403         1.280404   \n",
       "3          1.436774     0.000290     0.006083         1.192878   \n",
       "4          0.493513     0.002882     0.040346         1.776102   \n",
       "...             ...          ...          ...              ...   \n",
       "1331       1.151888     0.002012     0.063380         1.857919   \n",
       "1332       1.552830     0.002408     0.161349         1.659495   \n",
       "1333       0.555766     0.000000     0.096000         1.751533   \n",
       "1334       0.770643     0.000202     0.004445         1.835041   \n",
       "1335       0.490373     0.000000     0.045198         1.773180   \n",
       "\n",
       "      vcg_clause_mean  ...  gsat_FirstLocalMinStep_CoeffVariance  \\\n",
       "0            0.011143  ...                              0.210148   \n",
       "1            0.018012  ...                              0.124438   \n",
       "2            0.001760  ...                              0.066708   \n",
       "3            0.000968  ...                              0.053628   \n",
       "4            0.007656  ...                              0.086841   \n",
       "...               ...  ...                                   ...   \n",
       "1331         0.006701  ...                              0.079108   \n",
       "1332         0.004427  ...                              0.062664   \n",
       "1333         0.026000  ...                              0.140708   \n",
       "1334         0.000764  ...                              0.032571   \n",
       "1335         0.018442  ...                              0.120101   \n",
       "\n",
       "      gsat_FirstLocalMinStep_Median  gsat_FirstLocalMinStep_Q.10  \\\n",
       "0                              50.0                         42.0   \n",
       "1                              34.0                         29.0   \n",
       "2                             102.0                         94.0   \n",
       "3                             192.0                        179.0   \n",
       "4                              72.0                         64.0   \n",
       "...                             ...                          ...   \n",
       "1331                           81.0                         73.0   \n",
       "1332                          136.0                        125.0   \n",
       "1333                           26.0                         21.0   \n",
       "1334                          461.0                        441.0   \n",
       "1335                           35.0                         30.0   \n",
       "\n",
       "      gsat_FirstLocalMinStep_Q.90  gsat_BestAvgImprovement_Mean  \\\n",
       "0                            57.0                      0.954568   \n",
       "1                            40.0                      1.693036   \n",
       "2                           111.0                      0.398129   \n",
       "3                           205.0                      0.247528   \n",
       "4                            80.0                      0.822829   \n",
       "...                           ...                           ...   \n",
       "1331                         89.0                      0.781888   \n",
       "1332                        147.0                      0.776364   \n",
       "1333                         30.0                      2.093007   \n",
       "1334                        479.0                      0.202805   \n",
       "1335                         41.0                      1.982299   \n",
       "\n",
       "      gsat_BestAvgImprovement_CoeffVariance  gsat_FirstLocalMinRatio_Mean  \\\n",
       "0                                  0.591296                      1.141656   \n",
       "1                                  0.244951                      0.969015   \n",
       "2                                  0.824694                      0.935730   \n",
       "3                                  0.702251                      0.923327   \n",
       "4                                  0.209989                      0.855568   \n",
       "...                                     ...                           ...   \n",
       "1331                               0.256505                      0.837929   \n",
       "1332                               0.174557                      0.855907   \n",
       "1333                               0.117640                      1.000000   \n",
       "1334                               0.220101                      0.856952   \n",
       "1335                               0.141661                      0.995087   \n",
       "\n",
       "      gsat_FirstLocalMinRatio_CoeffVariance  gsat_EstACL_Mean     label  \n",
       "0                                  3.197217      9.240515e+09      gpmc  \n",
       "1                                  0.029930      5.401642e+03        d4  \n",
       "2                                  0.092714      3.561823e+04      gpmc  \n",
       "3                                  0.026977      1.268929e+05      gpmc  \n",
       "4                                  0.045802      1.647598e+04        d4  \n",
       "...                                     ...               ...       ...  \n",
       "1331                               0.066291      1.643030e+04  sharpsat  \n",
       "1332                               0.031698      3.816192e+04      gpmc  \n",
       "1333                               0.000000      3.168220e+03      gpmc  \n",
       "1334                               0.016884      6.236767e+05      gpmc  \n",
       "1335                               0.008575      6.045684e+03      gpmc  \n",
       "\n",
       "[1336 rows x 73 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://github.com/andvise/DM_Assignment/blob/main/train_data.csv?raw=true\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N8MCvTYTKw4Q",
    "outputId": "4a486e8c-c1fc-4d66-b112-a6c440857447"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "gpmc        921\n",
       "d4          168\n",
       "ganak       140\n",
       "addmc        55\n",
       "sharpsat     52\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label or target variable\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTvkBPQvITf-"
   },
   "source": [
    "\n",
    "\n",
    "## Basic models and evaluation \n",
    "\n",
    "### Baseline k-NN Model Performance\n",
    "\n",
    "\n",
    "Using Scikit-learn, train and evaluate a k-NN classifier using 70% of the dataset from training and 30% for testing. For this part of the project, we are not interested in optimising the parameters; we just want to get an idea of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zl0VXO0YH1nG",
    "outputId": "a78d9433-a029-44ef-d58e-8a1dc1484521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6733167082294265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6733167082294265"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Separate features and target variable\n",
    "\n",
    "X = df.drop('label', axis=1)  # Features\n",
    "y = df['label']               # Labels\n",
    "\n",
    "# Split the data into a training set and a test set (70% training, 30% testing)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, random_state = 42)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the k-NN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Train the classifier using the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model by comparing the predicted labels against the actual labels in the test set\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Alternatively, check accuracy using the score method\n",
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zADpr0f8IcGL"
   },
   "source": [
    "## Robust evaluation \n",
    "\n",
    "In this section, we are interested in more rigorous techniques by implementing more sophisticated methods. Trying to improve the k-NN classifier performances on this dataset.\n",
    "\n",
    "For instance, you could consider:\n",
    "* Hold-out and cross-validation.\n",
    "* Hyper-parameter tuning.\n",
    "* Feature selection.\n",
    "* Feature normalisation.\n",
    "* Etc.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MxOPyIYYML0Z"
   },
   "outputs": [],
   "source": [
    "# Import and load all necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, SelectPercentile\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tvBZH6ilInsA"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "df = pd.read_csv(\"https://github.com/andvise/DM_Assignment/blob/main/train_data.csv?raw=true\")\n",
    "\n",
    "df['label'].replace({'gpmc': 0, 'd4': 1, 'ganak': 2, 'addmc': 3, 'sharpsat': 4}, inplace=True)\n",
    "\n",
    "# Step 2: Prepare features and target\n",
    "X = df.drop('label', axis=1)\n",
    "y = df['label']\n",
    "\n",
    "# Step 3: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6FW9ASkTXv6"
   },
   "source": [
    "**Feature Normalisation Methods**\n",
    "\n",
    "MinMaxScaler and StandardScaler were both taken into account.\n",
    "\n",
    "K-NN computes the distances between data points and is sensitive to their magnitudes. One characteristic may dominate the distance calculations and produce biased results if it has a wider range of values than other features.\n",
    "Since MinMaxScaler scales data inside a constrained range [0, 1], it is a good choice for data with rigorous upper and lower bounds. For this reason, it was tested.\n",
    "\n",
    "Since StandardScaler normalizes data to have a zero mean and a one standard deviation, it can be used with data that has a Gaussian distribution, which is why it was also evaluated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AoeiqZDNP7aa",
    "outputId": "8c728931-b520-4227-a969-959d79801cfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy with standardized features: 0.7182044887780549\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Feature normalization using Standard scaler\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 5: Initialize and train the k-NN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "baseline_accuracy = knn.score(X_test_scaled, y_test)\n",
    "print('Baseline accuracy with standardized features:', baseline_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Xh_3edivRcl",
    "outputId": "21f314ef-c633-4130-e20f-d036a38fe4e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with MinMax Scaling: 0.7281795511221946\n"
     ]
    }
   ],
   "source": [
    " # Feature normalization using MinMax scaler\n",
    "\n",
    "# Applying MinMax Scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train k-NN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "print('Accuracy with MinMax Scaling:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yDDYfsgTBqa"
   },
   "source": [
    "## Dimensionality Reduction and Feature Selection\n",
    "\n",
    "**Feature Selection Methods**\n",
    "\n",
    "We can lower the likelihood of overfitting by limiting the amount of features, which is important for a distance-based technique like k-NN.Faster training and prediction times might result from fewer features.Using cross-validation, the effect of each approach on the accuracy of the k-NN model was quantified. The approach to feature selection that yielded the best balance between accuracy and simplicity of the model was selected.\n",
    "\n",
    "Select K  Best: Utilizing statistical tests that gauge the features' connection with the target variable, SelectKBest was utilized to choose the features.This approach offers a clear justification for feature selection based on statistical importance and is simple to use and comprehend.Features with the highest predictive potential for the result are identified by SelectKBest. In order to lower noise and increase model accuracy, this is essential.\n",
    "\n",
    "PCA : PCA was thought to preserve the most informative variance in the data while reducing dimensionality.The curse of dimensionality, in which the volume of space grows so quickly that the available data become sparse, can be lessened by reducing the number of dimensions, which can also significantly shorten the time it takes for k-NN to calculate distances.\n",
    "\n",
    "Select Percentile: A subset of the most pertinent features can be the focus of the k-NN model by utilizing SelectPercentile, which may improve model performance, particularly in datasets where the proportion of predictive features to total features is unknown.Using a given scoring function, this method chooses the best features that fall inside a particular percentile of the highest scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X8tJzEj1v37l",
    "outputId": "7f4b62c1-5144-4b6a-896f-b5d30ccc2342"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with PCA: 0.7406483790523691\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Feature selection methods\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "pca = PCA(n_components=0.95)  # keep 95% of variance\n",
    "\n",
    "# Fit the PCA model to the scaled training data and transform the training data into its principal components.\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Transform the scaled test data into the same principal component space defined by the training data.\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Fit the k-NN classifier on the training data transformed by PCA.\n",
    "knn.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predict the labels of the test data transformed by PCA using the trained k-NN classifier.\n",
    "y_pred_pca = knn.predict(X_test_pca)\n",
    "\n",
    "print('Accuracy with PCA:', accuracy_score(y_test, y_pred_pca))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oNp-KFliL_8G",
    "outputId": "84646c69-dad4-422a-c988-d63e550e5126"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with SelectKBest: 0.7256857855361596\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Initialize SelectKBest to select the top 10 features based on the ANOVA F-test.\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "\n",
    "\n",
    "X_train_kbest = selector.fit_transform(X_train_scaled, y_train)\n",
    "X_test_kbest = selector.transform(X_test_scaled)\n",
    "\n",
    "knn.fit(X_train_kbest, y_train)\n",
    "\n",
    "# Fit the k-NN classifier on the training data that has been transformed to only include the top 10 features.\n",
    "accuracy = knn.score(X_test_kbest, y_test)\n",
    "print('Accuracy with SelectKBest:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqLOJTPFMAJ_",
    "outputId": "af644d05-c178-4bd8-83e5-2f35ca3fd498"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with SelectPercentile: 0.7231920199501247\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "# Initialize SelectPercentile to keep only the top 10% of features based on the ANOVA F-test.\n",
    "percentile_selector = SelectPercentile(f_classif, percentile=10)\n",
    "\n",
    "X_train_percentile = percentile_selector.fit_transform(X_train_scaled, y_train)\n",
    "X_test_percentile = percentile_selector.transform(X_test_scaled)\n",
    "\n",
    "knn.fit(X_train_percentile, y_train)\n",
    "\n",
    "accuracy = knn.score(X_test_percentile, y_test)\n",
    "print('Accuracy with SelectPercentile:', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wu3tfxm1TnFV"
   },
   "source": [
    "## Hyperparameter Tuning using GridSearchCV\n",
    "\n",
    "**Hyper Parameter Tuning**\n",
    "\n",
    "GridSearchCV:  The performance of k-NN can be strongly influenced by the weight function, the distance metric, and the number of neighbors (k). GridSearchCV facilitates comprehensive exploration inside a designated parameter grid to identify the optimal amalgamation.'n_neighbors', 'weights', and'metrics' values were evaluated on a grid. The configuration that had the best accuracy during cross-validation was chosen. This stage guarantees that the model is well-suited to the training set and has good cross-validation generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XAknOoGlQFLc",
    "outputId": "c867cf33-81af-4823-8e72-a87b08b26652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.7382\n",
      "Best parameters: {'metric': 'manhattan', 'n_neighbors': 7, 'weights': 'distance'}\n",
      "Best cross-validation score: 0.7614973262032085\n",
      "Tuned model accuracy: 0.7381546134663342\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Hyperparameter tuning using GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],                    # Lists different values for 'n_neighbors' to evaluate\n",
    "    'weights': ['uniform', 'distance'],              # Evaluates both uniform and distance-based weighting\n",
    "    'metric': ['euclidean', 'manhattan']             # Evaluates both Euclidean and Manhattan distance metrics\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with the k-NN classifier, the parameter grid, and the scoring method.\n",
    "# The cv parameter defines the number of cross-validation folds (5 in this case).\n",
    "\n",
    "grid_search = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "\n",
    "# Perform the grid search over the specified parameter grid using the training data.grid_search.fit(X_train_scaled, y_train)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_knn = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model found by GridSearchCV on the scaled test data.\n",
    "tuned_accuracy = best_knn.score(X_test_scaled, y_test)\n",
    "\n",
    "# Predict on the test set using the best found parameters\n",
    "y_pred = grid_search.predict(X_test_scaled)\n",
    "\n",
    "# Calculate and print the accuracy on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Test set accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "print('Tuned model accuracy:', tuned_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFpZSguZiCCQ"
   },
   "source": [
    "## Model Evaluation using Cross-Validation and Hold-Out Test Set\n",
    "\n",
    "**k-Fold Cross-Validation, Hold-Out Method**\n",
    "\n",
    "By estimating model performance more robustly across many data subsets, cross-validation keeps the model from becoming overly specialized to one particular data partition.After all optimizations, the hold-out approach was applied to evaluate the model's performance on entirely unseen data, hence emulating the model's performance in real-world scenarios.The assessment of the stability and reliability of the model was conducted by monitoring the variations in performance during cross-validation on several data folds, and subsequently on a hold-out set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QksPLGCEQkJv",
    "outputId": "3fd2724d-067e-45f5-a535-85abaf23d765"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation average accuracy: 0.7614973262032085\n"
     ]
    }
   ],
   "source": [
    "# Step 8 : Cross Validation\n",
    "\n",
    "cv_scores = cross_val_score(best_knn, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "cv_average_accuracy = cv_scores.mean()\n",
    "print('Cross-validation average accuracy:', cv_average_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "twS0rvwTNin_",
    "outputId": "1792683c-da80-4706-d0ad-504af987c546"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold-out set accuracy:  0.7381546134663342\n"
     ]
    }
   ],
   "source": [
    "# Holdout Validation\n",
    "\n",
    "# best_knn is the classifier from GridSearchCV with optimal settings\n",
    "best_knn = grid_search.best_estimator_\n",
    "\n",
    "# Fit on the entire training data\n",
    "best_knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate on the hold-out set\n",
    "holdout_accuracy = best_knn.score(X_test_scaled, y_test)\n",
    "print(\"Hold-out set accuracy: \", holdout_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YH6vhUeATxnn"
   },
   "source": [
    "**IMPROVISED knn CLASSIFIER MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvBGHMRRpYVn"
   },
   "source": [
    "**MODEL 1**\n",
    "\n",
    "### Model 1: PCA + k-NN Pipeline\n",
    "\n",
    "\n",
    "Feature and Target Separation: To enable the supervised learning strategy required for k-NN classification, the dataset was split into feature set X and target y.\n",
    "\n",
    "Train-Test Split: To guarantee reproducibility, we divided the data using a random seed into training (70%) and testing (30%) groups. This split is essential for evaluating the model's generalizability because it validates the model's performance on untested data.\n",
    "\n",
    "Standard Scaling: To ensure that every feature contributes equally to the distance calculations—a need for the efficacy of k-NN—StandardScaler was applied to equalize the feature data.\n",
    "\n",
    "PCA for Dimensionality Reduction: To decrease the number of features while keeping 95% of the data variance, PCA(n_components=0.95) was used. By doing this step, the computational cost and curse of dimensionality are lessened.\n",
    "\n",
    "Pipeline Setup: To expedite the preprocessing and classification procedures, a pipeline consisting of the StandardScaler, PCA, and KNeighborsClassifier was constructed.\n",
    "\n",
    "Hyperparameter tuning with GridSearchCV: Using a parameter grid that changed the number of neighbors, weights, and distance metrics throughout a 5-fold cross-validation, GridSearchCV was utilized to find the best parameters for the k-NN classifier. The selection of hyperparameters that result in the optimum cross-validation accuracy is ensured by this thorough search.\n",
    "\n",
    "Model Performance : An accuracy of 73.8 % is obtained with the model on test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VyoAI6Ly3fka",
    "outputId": "4f7c81f1-a098-443e-fa5d-043aa915200b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best parameters: {'knn__metric': 'euclidean', 'knn__n_neighbors': 7, 'knn__weights': 'distance'}\n",
      "Accuracy on test set: 0.7381546134663342\n",
      "Cross-validation scores: [0.76470588 0.71122995 0.75935829 0.70053476 0.78074866]\n",
      "Best cross-validation score: 0.7433155080213905\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Setup the pipeline with the necessary transformations and the estimator\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),      # Scale features to normalize data, essential for k-NN\n",
    "    ('pca', PCA(n_components=0.95)),   # Reduce dimensionality to retain 95% variance which helps in alleviating the curse of dimensionality\n",
    "    ('knn', KNeighborsClassifier())    # k-NN classifier, parameters to be tuned via GridSearch\n",
    "])\n",
    "\n",
    "# Define the grid of parameters to search\n",
    "param_grid = {\n",
    "    'knn__n_neighbors': [3, 5, 7, 9],           # Number of neighbors to use\n",
    "    'knn__weights': ['uniform', 'distance'],    # Weighting type: uniform or distance\n",
    "    'knn__metric': ['euclidean', 'manhattan']   # Distance metric: Euclidean or Manhattan\n",
    "\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with the pipeline and the parameter grid\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Train the model with GridSearchCV to find the best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output the best parameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best model on the testing set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on test set:\", accuracy)\n",
    "\n",
    "\n",
    "# Cross-validation for performance estimation\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5)\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1MIFxR3rAAF"
   },
   "source": [
    "**FINAL ROBUST MODEL WITH IMPROVED k-NN CLASSIFIER PERFORMANCE ON THIS DATASET**\n",
    "\n",
    "###  Model 2: SelectKBest + k-NN Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3a7ysenfy66y"
   },
   "source": [
    "**Train-Test Split:** To ensure a random and repeatable distribution of data points, the dataset was split into training (70%) and testing (30%) subsets using train test split. This division facilitates the model's assessment on hypothetical data, which is essential for determining the model's practicality.\n",
    "\n",
    "**Model Setup Pipeline Configuration:**\n",
    "\n",
    "**MinMaxScaler:** Used to normalize feature values between 0 and 1, reducing the impact of outlier values and guaranteeing that each feature makes an equal contribution to the distance computations that are essential to k-NN models.\n",
    "\n",
    "**SelectKBest and f_classif:** Using SelectKBest and f_classif, the model is trained to focus on the most pertinent features for target prediction by choosing the top k features from the ANOVA F-test. This eliminates dimensionality and could improve the accuracy of the model by removing superfluous information.\n",
    "\n",
    "**KNeighborsClassifier:** Set up as the last stage of the pipeline, ready for fine-tuning to determine the ideal values for the distance measure, weighting scheme, and neighborhood size.\n",
    "\n",
    "**Hyperparameter Tuning GridSearchCV:** A thorough grid search was carried out to investigate different setups of the k-NN classifier's hyperparameters and feature selection\n",
    "\n",
    "\n",
    "**Number of Features (k in SelectKBest)**:  Varied to evaluate how various feature subset sizes affected the performance of the model.You may systematically investigate how altering the number of features impacts the model's performance by experimenting with different values of k. This is crucial since adding too many features could result in noise and redundancy, overfitting, and higher computing costs. The range of values [5, 10, 15, 20] makes it possible to evaluate this balance in various contexts, which aids in locating the sweet spot where the model performs best in terms of generalization.\n",
    "\n",
    "**Neighbor Count (knn__n_neighbors):**\n",
    "Values: [3, 5, 7, 9, 11,]. In k-NN, the number of neighbors is an important parameter. A model that has too few neighbors may be too sensitive to noise in the training set (overfitting), whereas a model with too many neighbors may be too broadly generalized (underfitting). It is possible to identify a balanced value that offers the highest generalization performance by testing several values.The ideal neighborhood size for classification was determined by testing a range of neighbor counts.\n",
    "\n",
    "\n",
    "Weights and Metrics: To determine the optimal method for calculating distance in k-NN, uniform and distance-based weights as well as Euclidean and Manhattan metrics were tested.\n",
    "\n",
    "**Knn__weights, or weights**: ['uniform', 'distance']\n",
    "\n",
    "This parameter establishes whether closer neighbors have a stronger influence on the prediction or if each point in the neighborhood contributes uniformly. In clusters with different densities, distance weighting might be very helpful as uniform weighting might not work as well. increases the model's capacity to adjust to local data structures, which can result in higher accuracy—particularly in cases where class distributions are intricate and overlap.\n",
    "\n",
    "**Metric of Distance (knn__metric)**:['euclidean','manhattan']\n",
    "\n",
    "Since different metrics can capture different features of data similarity, the choice of distance metric can have a major impact on the performance of k-NN. While Manhattan distance can be more reliable in high-dimensional spaces or situations where the data isn't isotropically distributed, Euclidean distance is the standard.\n",
    "\n",
    "\n",
    "**Cross Validation:** To make sure that the model generalizes effectively across various data splits and that the tuning procedure is resilient against overfitting, the search was conducted over a 5-fold cross-validation.Cross-validation models have a higher chance of generalizing well to new data. Through the validation procedure, it is made sure that the hyperparameters are optimized for a variety of training scenarios, not just one particular training set.Cross-validation becomes an effective tool for hyperparameter tweaking when paired with GridSearchCV. Every possible combination of hyperparameters is tested across all folds, and the one with the best average performance is chosen.\n",
    "\n",
    "**cv = 5** : The number of folds in a (Stratified)KFold cross-validation is indicated by the parameter cv=5. It determines how the data is divided up for assessment. When five parts of the dataset are utilized in 5-fold cross-validation, four of the parts are used for training and one for validation throughout each cycle. To ensure that every component is utilized as the validation set once, this step is repeated five times.\n",
    "\n",
    "**scoring='accuracy'**: Specifies the performance metric to be applied for assessing the effectiveness of the combinations of hyperparameters. When 'accuracy' is selected, GridSearchCV will determine the optimal hyperparameters based on the validation set's accuracy score.\n",
    "\n",
    "\n",
    "**Performance Reporting:** Cross-validation scores and testing the model on a separate test set offer an additional layer of assurance regarding the model's stability and performance after the ideal parameters have been determined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UJNSnYiMn1nP",
    "outputId": "91e28ad2-f0e3-43c6-9226-3bd8b2bc38dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters (SelectKBest): {'knn__metric': 'euclidean', 'knn__n_neighbors': 11, 'knn__weights': 'distance', 'selector__k': 15}\n",
      "Accuracy on test set: 0.7506234413965087\n",
      "Cross-validation scores: [0.77540107 0.75935829 0.7486631  0.72727273 0.79144385]\n",
      "Best cross-validation score (SelectKBest): 0.7604278074866311\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Setup the pipeline with MinMaxScaler, SelectKBest for feature selection, and KNeighborsClassifier\n",
    "pipeline_kbest = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('selector', SelectKBest(f_classif)),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Define the grid of parameters to search\n",
    "param_grid_kbest = {\n",
    "    'selector__k': [5, 10, 15, 20],                # Trying different numbers of top features\n",
    "    'knn__n_neighbors': [3, 5, 7, 9, 11],          # Number of neighbors to use\n",
    "    'knn__weights': ['uniform', 'distance'],       # Weighting type: uniform or distance\n",
    "    'knn__metric': ['euclidean', 'manhattan']      # Distance metric: Euclidean or Manhattan\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_kbest = GridSearchCV(pipeline_kbest, param_grid_kbest, cv=5, scoring='accuracy')\n",
    "grid_search_kbest.fit(X_train, y_train)\n",
    "\n",
    "# Output best parameters\n",
    "print(\"Best parameters (SelectKBest):\", grid_search_kbest.best_params_)\n",
    "\n",
    "# Evaluate the best model on the testing set\n",
    "best_model = grid_search_kbest.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on test set:\", accuracy)\n",
    "\n",
    "\n",
    "# Cross-validation for performance estimation\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5)\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Best cross-validation score (SelectKBest):\", grid_search_kbest.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsEQFrzPRcPW"
   },
   "source": [
    "**INFERENCE ON BEST MODEL:**\n",
    "\n",
    "The top 20 characteristics chosen by SelectKBest, the manhattan distance metric, distance weighting, and ten neighbors were all used in the top-performing model. According to this setup, the best accuracy is obtained when a more granular feature selection is paired with a larger neighborhood that has distance-weighted contributions.\n",
    "\n",
    "Test Set Performance: The model obtained an accuracy of roughly 75.06% on the test data that was not visible. The close correlation shown between test accuracy and cross-validation highlights the efficacy and generalizability of the approach.\n",
    "\n",
    "Comparing Model 2 to Model 1, there is a little improvement. This is explained by SelectKBest's ability to effectively pick the most pertinent features, which may give the k-NN algorithm a clearer signal and increase accuracy.Model 2 employs SelectKBest, which chooses features based on statistical tests, whereas Model 1 uses principle Component Analysis (PCA), which shrinks the feature space by converting features into principle components.\n",
    "While SelectKBest keeps the original characteristics, which may be easier to read, PCA may delete or modify features that are helpful for categorization.\n",
    "\n",
    "Model 2 has more neighbors (11 vs. 7) than Model 1, which implies that adding more local information (without overfitting) improves performance, especially when the appropriate features are chosen.\n",
    "The difference in scaling techniques may also have an impact on the improvement from Model 1 to Model 2, suggesting that the MinMaxScaler may be a better fit for this dataset than the StandardScaler.\n",
    "\n",
    "\n",
    "Focusing on the most statistically relevant characteristics, SelectKBest's incorporation into the k-NN classification pipeline significantly improved the model's predicted accuracy. The model's setup was further adjusted by a methodical investigation of hyperparameter space using GridSearchCV, resulting in a robust classifier that exhibits consistent performance across several data segments. The model's sophisticated knowledge of feature interactions was demonstrated by the use of Manhattan distance, which improved classification accuracy, especially when combined with a bigger collection of neighbors and distance-based weighting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYoMg0EZIrNd"
   },
   "source": [
    "## New classifier \n",
    "\n",
    "### Alternative Model: SVM Classifier with PCA\n",
    "\n",
    "\n",
    "Replicating the previous task for a classifier different than K-NN and decision trees. \n",
    "Trying to create the best model for the given dataset and the choice is briefly described.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qeejpjwl5OHt",
    "outputId": "8881dcba-bcda-47a8-d3d2-8f5c79e2eecd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'svm__C': 100, 'svm__gamma': 'auto', 'svm__kernel': 'rbf'}\n",
      "Accuracy on test set: 0.7381546134663342\n",
      "Cross-validation scores: [0.76470588 0.7540107  0.7486631  0.68449198 0.77540107]\n",
      "Best cross-validation score: 0.7454545454545454\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"https://github.com/andvise/DM_Assignment/blob/main/train_data.csv?raw=true\")\n",
    "X = df.drop('label', axis=1)\n",
    "y = df['label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Setup the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),     # Normalize data\n",
    "    ('pca', PCA(n_components=0.95)),  # Dimensionality reduction\n",
    "    ('svm', SVC())                    # SVM classifier\n",
    "])\n",
    "\n",
    "# Define the grid of parameters to search\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],               # Regularization parameter\n",
    "    'svm__kernel': ['linear', 'poly', 'rbf'],  # Type of kernel used in SVM\n",
    "    'svm__gamma': ['scale', 'auto']            # Kernel coefficient for 'rbf'\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output best parameters and the best cross-validation score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best model on the testing set\n",
    "model = grid_search.best_estimator_\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on test set:\", accuracy)\n",
    "\n",
    "# Cross-validation for performance estimation\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcZDoQ3DiSFb"
   },
   "source": [
    "##  Final Model Inference and Conclusions\n",
    "\n",
    "**Model Selection: Support Vector Machine (SVM)**\n",
    "\n",
    "For addressing this classification task, the SVM classifier offers a strong foundation with plenty of room for additional improvements to improve performance. The SVM classifier performs well on the dataset after being carefully tuned through a process of hyperparameter testing and assessed using cross-validation and a hold-out test set. The SVM is a great option for this classification problem because of its adaptability, efficiency in high-dimensional spaces, and resilience to overfitting.\n",
    "\n",
    "SVMs' efficiency in medium-sized datasets is relatively reasonable, despite the fact that they can be computationally demanding when working with very big datasets. This is especially true when combined with feature reduction techniques like PCA, as seen in the pipeline.By removing noise from the dataset, PCA's dimensionality reduction technique helps to streamline the dataset, speed up training, and maybe enhance classifier performance.Using the 'rbf' kernel, for example, SVM can represent non-linear boundaries, which makes it an effective tool for capturing intricate correlations in data.\n",
    "\n",
    "**svm__C:** [0.1, 1, 10, 100]: In support of support vector machines (SVMs), the 'C' parameter regulates the trade-off between obtaining a low error on the training data and guaranteeing a smooth decision boundary. A high 'C' tries to categorize all training instances properly by allowing the model to choose more support vectors, whereas a low 'C' smoothes the decision surface.\n",
    "\n",
    "**svm__kernel:** ['poly', 'rbf', 'linear']: The purpose of selecting this set of kernels is to investigate both non-linear ('poly' and 'rbf') and linear ('linear') correlations in the data. While the 'rbf' kernel can handle an infinite number of dimensions, which is useful for complex patterns, the 'poly' kernel examines higher-dimensional spaces.\n",
    "\n",
    "**svm__gamma:** ['scale', 'auto']: This option establishes the extent to which the impact of a solitary training sample is felt. High values indicate \"close,\" and low ones indicate \"far.\" The gamma value is 1 / (n_features * X.var()) for the'scale' option and 1 / n_features for the 'auto' option. The classifier's capacity to manage the bias-variance trade-off may be impacted by this.\n",
    "\n",
    "\n",
    "The classifier's robustness and efficacy are improved by its margin maximization approach, which also enables it to handle complicated datasets and adapt to different data architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We explored multiple pipelines for classifying the optimal #SAT solver:\n",
    "- k-NN baseline achieved ~67% accuracy.\n",
    "- After feature scaling and PCA, accuracy improved to ~74%.\n",
    "- Using SelectKBest for feature selection further improved performance to ~75%.\n",
    "- GridSearchCV optimized hyperparameters such as `n_neighbors`, `weights`, and `distance metric`.\n",
    "- The final k-NN model (with MinMaxScaler + SelectKBest + GridSearchCV) reached 75.06% test accuracy.\n",
    "- An SVM classifier with PCA achieved comparable performance (~73.8%) and robust cross-validation scores.\n",
    "\n",
    " **Conclusion**: The refined k-NN model outperformed the SVM, showing that feature-based selection combined with distance-weighted k-NN offers superior classification for #SAT solver selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
